# Metrics Module Optimization Summary

## ä¼˜åŒ–æ¦‚è¿°

æœ¬æ¬¡ä¼˜åŒ–å¯¹ `habit/core/machine_learning/evaluation/metrics.py` æ¨¡å—è¿›è¡Œäº†å…¨é¢æ”¹è¿›ï¼Œåœ¨ä¿æŒå‘åå…¼å®¹çš„å‰æä¸‹ï¼Œå®ç°äº†æ€§èƒ½æå‡å’ŒåŠŸèƒ½å¢å¼ºã€‚

## ä¸»è¦ä¼˜åŒ–

### 1. æ€§èƒ½ä¼˜åŒ–ï¼šæ··æ·†çŸ©é˜µç¼“å­˜ ğŸš€

**é—®é¢˜**ï¼šä¹‹å‰æ¯ä¸ªæŒ‡æ ‡å‡½æ•°éƒ½ç‹¬ç«‹è®¡ç®—æ··æ·†çŸ©é˜µï¼Œå¯¼è‡´é‡å¤è®¡ç®—8æ¬¡ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥ `MetricsCache` ç±»

```python
class MetricsCache:
    """Cache confusion matrix to avoid repeated calculations."""
    @property
    def confusion_matrix(self):
        if self._cm is None:
            self._cm = metrics.confusion_matrix(self.y_true, self.y_pred)
        return self._cm
```

**æ€§èƒ½æå‡**ï¼šçº¦ **8å€** é€Ÿåº¦æå‡ï¼ˆä»8æ¬¡æ··æ·†çŸ©é˜µè®¡ç®—é™è‡³1æ¬¡ï¼‰

**ä½¿ç”¨æ–¹æ³•**ï¼š
```python
# å¯ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ï¼‰
metrics = calculate_metrics(y_true, y_pred, y_prob, use_cache=True)

# ç¦ç”¨ç¼“å­˜
metrics = calculate_metrics(y_true, y_pred, y_prob, use_cache=False)
```

---

### 2. æ‰©å±•Target Metricsæ”¯æŒ ğŸ’¡

**æ–°å¢æ”¯æŒçš„æŒ‡æ ‡**ï¼š
- âœ… Sensitivityï¼ˆå·²æ”¯æŒï¼‰
- âœ… Specificityï¼ˆå·²æ”¯æŒï¼‰
- âœ… **PPV (Precision)** - æ–°å¢
- âœ… **NPV** - æ–°å¢
- âœ… **F1-score** - æ–°å¢
- âœ… **Accuracy** - æ–°å¢

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
targets = {
    'sensitivity': 0.91,
    'specificity': 0.91,
    'ppv': 0.85,           # æ–°å¢
    'npv': 0.90,           # æ–°å¢
    'f1_score': 0.88       # æ–°å¢
}

result = calculate_metrics_at_target(y_true, y_pred_proba, targets)
```

**æ³¨æ„**ï¼šPPV/NPV/F1ç­‰æŒ‡æ ‡éœ€è¦éå†æ‰€æœ‰é˜ˆå€¼è®¡ç®—ï¼Œæ€§èƒ½å¼€é”€è¾ƒå¤§ï¼Œä½†å¿…è¦ã€‚

---

### 3. Fallbackæœºåˆ¶ï¼šæœ€æ¥è¿‘é˜ˆå€¼ ğŸ¯

**é—®é¢˜**ï¼šå½“æ²¡æœ‰é˜ˆå€¼èƒ½åŒæ—¶æ»¡è¶³æ‰€æœ‰ç›®æ ‡æ—¶ï¼ˆå¦‚ç›®æ ‡è¿‡é«˜ï¼‰ï¼Œä¹‹å‰ä¼šç›´æ¥è¿”å›ç©ºç»“æœã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šè‡ªåŠ¨å¯»æ‰¾"æœ€æ¥è¿‘"çš„é˜ˆå€¼

```python
result = calculate_metrics_at_target(
    y_true, y_pred_proba,
    {'sensitivity': 0.99, 'specificity': 0.99},  # ä¸å¯èƒ½åŒæ—¶æ»¡è¶³
    fallback_to_closest=True,
    distance_metric='euclidean'  # 'euclidean', 'manhattan', or 'max'
)

# è¿”å›ç»“æ„
{
    'closest_threshold': {
        'threshold': 0.5234,
        'metrics': {...},
        'distance_to_target': 0.0523,
        'satisfied_targets': ['sensitivity'],
        'unsatisfied_targets': ['specificity'],
        'warning': 'No threshold satisfies all targets. This is the closest match.'
    }
}
```

**è·ç¦»åº¦é‡**ï¼š
- `euclidean`: âˆšÎ£(actual - target)Â²ï¼ˆé»˜è®¤ï¼‰
- `manhattan`: Î£|actual - target|
- `max`: max(|actual - target|)

**ä¸¥æ ¼é¿å…æ•°æ®æ³„éœ²**ï¼š
- è®­ç»ƒé›†ï¼šæ‰¾åˆ°æœ€æ¥è¿‘é˜ˆå€¼
- æµ‹è¯•é›†ï¼šåº”ç”¨è®­ç»ƒé›†çš„é˜ˆå€¼ï¼ˆä¸é‡æ–°æœç´¢ï¼‰

---

### 4. æ™ºèƒ½é˜ˆå€¼é€‰æ‹©ç­–ç•¥ ğŸ§ 

**é—®é¢˜**ï¼šå½“å¤šä¸ªé˜ˆå€¼éƒ½æ»¡è¶³æ¡ä»¶æ—¶ï¼Œå¦‚ä½•é€‰æ‹©æœ€ä¼˜ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼šä¸‰ç§ç­–ç•¥

#### ç­–ç•¥1ï¼šFirstï¼ˆå¿«é€Ÿï¼‰
```python
result = calculate_metrics_at_target(..., threshold_selection='first')
# è¿”å›ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„é˜ˆå€¼
```

#### ç­–ç•¥2ï¼šYoudenï¼ˆç»å…¸ï¼‰
```python
result = calculate_metrics_at_target(..., threshold_selection='youden')
# è¿”å›YoudenæŒ‡æ•°æœ€å¤§çš„é˜ˆå€¼
# Youden = Sensitivity + Specificity - 1
```

#### ç­–ç•¥3ï¼šPareto+Youdenï¼ˆæ¨èï¼‰â­
```python
result = calculate_metrics_at_target(..., threshold_selection='pareto+youden')
# 1. æ‰¾å‡ºæ‰€æœ‰Paretoæœ€ä¼˜é˜ˆå€¼ï¼ˆæ— æ³•è¢«å…¶ä»–é˜ˆå€¼"å®Œå…¨æ”¯é…"ï¼‰
# 2. åœ¨Paretoæœ€ä¼˜ä¸­é€‰æ‹©Youdenæœ€å¤§çš„
```

**è¿”å›ç»“æ„**ï¼š
```python
{
    'best_threshold': {
        'threshold': 0.5222,
        'metrics': {...},
        'strategy': 'pareto+youden',
        'youden_index': 0.8792,
        'pareto_optimal_count': 3  # Paretoæœ€ä¼˜é˜ˆå€¼æ•°é‡
    }
}
```

---

### 5. ç±»åˆ«ç­›é€‰åŠŸèƒ½ ğŸ“‹

**åŠŸèƒ½**ï¼šæŒ‰ç±»åˆ«ç­›é€‰è¦è®¡ç®—çš„æŒ‡æ ‡

```python
# åªè®¡ç®—åŸºç¡€æŒ‡æ ‡ï¼ˆå¿«é€Ÿï¼‰
basic_metrics = calculate_metrics(
    y_true, y_pred, y_prob,
    categories=['basic']
)
# è¿”å›ï¼šaccuracy, sensitivity, specificity, ppv, npv, f1_score, auc

# åªè®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
stat_metrics = calculate_metrics(
    y_true, y_pred, y_prob,
    categories=['statistical']
)
# è¿”å›ï¼šhosmer_lemeshow_p_value, spiegelhalter_z_p_value

# è®¡ç®—å¤šä¸ªç±»åˆ«
metrics = calculate_metrics(
    y_true, y_pred, y_prob,
    categories=['basic', 'statistical']
)

# è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆé»˜è®¤ï¼‰
all_metrics = calculate_metrics(y_true, y_pred, y_prob)
```

---

### 6. å¤šåˆ†ç±»æ”¯æŒå‡†å¤‡ ğŸ”®

**å½“å‰çŠ¶æ€**ï¼š
- âœ… AUCï¼šå·²æ”¯æŒå¤šåˆ†ç±»ï¼ˆOne-vs-Restï¼‰
- âœ… åŸºç¡€æŒ‡æ ‡ï¼šæ–°å¢å¤šåˆ†ç±»æ”¯æŒï¼ˆmacro averageï¼‰

**å¤šåˆ†ç±»ç¤ºä¾‹**ï¼š
```python
# äºŒåˆ†ç±»ï¼ˆç°æœ‰è¡Œä¸ºï¼‰
cm = [[TN, FP],
      [FN, TP]]
sensitivity = TP / (TP + FN)

# å¤šåˆ†ç±»ï¼ˆæ–°å¢ï¼‰
cm = [[c00, c01, c02],
      [c10, c11, c12],
      [c20, c21, c22]]
# Per-class recall, then macro average
```

---

## å‘åå…¼å®¹æ€§ âœ…

æ‰€æœ‰ç°æœ‰ä»£ç **æ— éœ€ä¿®æ”¹**ï¼Œé»˜è®¤è¡Œä¸ºä¿æŒä¸å˜ï¼š

```python
# æ—§ä»£ç ä»ç„¶æœ‰æ•ˆ
metrics = calculate_metrics(y_true, y_pred, y_prob)
result = calculate_metrics_at_target(y_true, y_prob, {'sensitivity': 0.9})

# æ–°åŠŸèƒ½é€šè¿‡å¯é€‰å‚æ•°å¯ç”¨
metrics = calculate_metrics(y_true, y_pred, y_prob, use_cache=True, categories=['basic'])
result = calculate_metrics_at_target(
    y_true, y_prob, targets,
    threshold_selection='pareto+youden',
    fallback_to_closest=True
)
```

---

## ä½¿ç”¨å»ºè®®

### æ¨èé…ç½®ï¼ˆæœ€ä½³å®è·µï¼‰

```python
# 1. è®­ç»ƒé›†ï¼šæ‰¾æœ€ä¼˜é˜ˆå€¼
train_result = calculate_metrics_at_target(
    y_train_true,
    y_train_prob,
    targets={'sensitivity': 0.91, 'specificity': 0.91, 'ppv': 0.85},
    threshold_selection='pareto+youden',  # æ™ºèƒ½é€‰æ‹©
    fallback_to_closest=True,            # å¯ç”¨fallback
    distance_metric='euclidean'
)

# 2. æå–é˜ˆå€¼
if train_result['best_threshold']:
    threshold = train_result['best_threshold']['threshold']
    logger.info(f"æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼: {threshold}")
elif train_result['closest_threshold']:
    threshold = train_result['closest_threshold']['threshold']
    logger.warning(f"ä½¿ç”¨æœ€æ¥è¿‘é˜ˆå€¼: {threshold}")
else:
    threshold = 0.5  # é»˜è®¤
    logger.error("æœªæ‰¾åˆ°åˆé€‚é˜ˆå€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼")

# 3. æµ‹è¯•é›†ï¼šåº”ç”¨è®­ç»ƒé›†é˜ˆå€¼
test_metrics = apply_threshold(y_test_true, y_test_prob, threshold)
```

---

## æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|-----|--------|--------|------|
| è®¡ç®—8ä¸ªåŸºç¡€æŒ‡æ ‡ | 8æ¬¡CMè®¡ç®— | 1æ¬¡CMè®¡ç®— | **8x** |
| Target metrics (sens+spec) | è¾ƒå¿« | è¾ƒå¿« | ~1x |
| Target metrics (+ppv+npv+f1) | N/A | ä¸­ç­‰ | æ–°åŠŸèƒ½ |
| Paretoæœ€ä¼˜é€‰æ‹© | N/A | ä¸­ç­‰ | æ–°åŠŸèƒ½ |

**æ¨è**ï¼š
- æ—¥å¸¸ä½¿ç”¨ï¼šå¯ç”¨ç¼“å­˜
- ç®€å•åœºæ™¯ï¼šåªç”¨sensitivity+specificity
- å¤æ‚åœºæ™¯ï¼šå¯å¢åŠ ppv/npv/f1ï¼ˆæ€§èƒ½æ¢ç²¾åº¦ï¼‰

---

## æµ‹è¯•éªŒè¯

è¿è¡Œæµ‹è¯•å¥—ä»¶ï¼š
```bash
pytest tests/test_metrics_optimization.py -v
```

æµ‹è¯•è¦†ç›–ï¼š
- âœ… æ··æ·†çŸ©é˜µç¼“å­˜æ€§èƒ½
- âœ… PPV/NPV/F1ç›®æ ‡æ”¯æŒ
- âœ… Fallbackæœºåˆ¶
- âœ… Pareto+Youdené€‰æ‹©
- âœ… ç±»åˆ«ç­›é€‰
- âœ… ä¸åŒè·ç¦»åº¦é‡

---

## å·²çŸ¥é™åˆ¶

1. **PPV/NPVè®¡ç®—æ…¢**ï¼šéœ€è¦éå†æ‰€æœ‰é˜ˆå€¼ï¼ŒO(n)å¤æ‚åº¦
   - å»ºè®®ï¼šä¼˜å…ˆç”¨sensitivity+specificityï¼Œå¿…è¦æ—¶æ‰åŠ ppv/npv

2. **Paretoç®—æ³•å¤æ‚åº¦**ï¼šO(nÂ²) worst case
   - å®é™…å½±å“å°ï¼ˆé˜ˆå€¼æ•°é‡é€šå¸¸<1000ï¼‰

3. **å¤šåˆ†ç±»å®Œå…¨æ”¯æŒ**ï¼šéœ€è¦æ›´å¤šæµ‹è¯•å’ŒéªŒè¯
   - å½“å‰ï¼šåŸºç¡€æ”¯æŒï¼ˆmacro averageï¼‰
   - æœªæ¥ï¼šweighted, per-classç­‰ç­–ç•¥

---

## æœªæ¥æ”¹è¿›æ–¹å‘

1. **GPUåŠ é€Ÿ**ï¼šæ··æ·†çŸ©é˜µè®¡ç®—ï¼ˆå¤§è§„æ¨¡æ•°æ®ï¼‰
2. **å¹¶è¡ŒåŒ–**ï¼šParetoæœ€ä¼˜æœç´¢ï¼ˆå¤šçº¿ç¨‹ï¼‰
3. **è‡ªé€‚åº”ç­–ç•¥**ï¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©threshold_selection
4. **å¯è§†åŒ–**ï¼šParetoå‰æ²¿æ›²çº¿ç»˜åˆ¶
5. **å¤šåˆ†ç±»å…¨é¢æ”¯æŒ**ï¼šweighted, per-classç­–ç•¥

---

## æŠ€æœ¯å€ºåŠ¡æ¸…ç†

å·²è§£å†³çš„æŠ€æœ¯å€ºï¼š
- âœ… é‡å¤è®¡ç®—æ··æ·†çŸ©é˜µï¼ˆ8xæ€§èƒ½æŸå¤±ï¼‰
- âœ… ç¡¬ç¼–ç åªæ”¯æŒsens/spec
- âœ… æ— fallbackæœºåˆ¶
- âœ… categoryå‚æ•°æœªä½¿ç”¨
- âœ… F1-scoreä½æ•ˆè®¡ç®—ï¼ˆ3æ¬¡CMï¼‰

---

## è”ç³»ä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤Issueæˆ–Pull Requestã€‚

**ä½œè€…**ï¼šHABITå¼€å‘å›¢é˜Ÿ  
**æ—¥æœŸ**ï¼š2026-01-25  
**ç‰ˆæœ¬**ï¼šv2.0
